# 情報理論

## 自己情報量

自己情報量 (選択情報量, 自己エントロピー)
: ある事象の起こりにくさを数値化した物

$$
自己情報量 I(x) = - \log_2 P(x)
$$

情報量の加法性
: 独立な事象A, Bが独立に起きるとき、事象「AもBも起きる」の情報量はAとBの情報量の和になる

$$
I(A, B) = -\log(A, B)\\
= -\log P(A) \cdot P(B)\\
= - [\log P(A) + \log P(B)]\\
= I(A) + I(B)
$$

## 平均情報量

平均情報量 (平均情報量、シャノン情報量、情報論のエントロピー)
: 情報量の期待値

$$
平均情報量 H = -\sum P(E) \log P(E)
$$

Q. こんなサイコロの情報量は？

| 確率変数X | 1 | 2 | 3 | 4 | 5 | 6 |
| --- | --- | --- | --- | --- | --- | --- |
| 確率P | 0.5 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 |

４が出た時の自己情報量：$- \log_2 \dfrac{1}{10} = \log_2 10$

平均情報量：$H=-(0.5 \times \log_2 0.5 + 0.1 \times \log_2 0.1 \times 5) = -0.5\log_20.5-0.5\log_2 0.1$

## 交差エントロピー

２つの確率分布がまったく同じ時に最小となる
分類問題の損失関数で用いられる。

## 結合エントロピー

離散型：$H(X,Y)=-\sum_{i=1}^{M_X}
\sum_{j=1}^{M_Y}
P(x_i, y_j) \log P(x_i, y_j)$

連続型：$H(X,Y) = \int P(x_i, y_j) \log P(x_i, y_j) \ dxdy$

## KLダイバージェンス

KLダイバージェンス（カルバック・ライブラーダイバージェンス）
: ２つの確率分布の近さを表現する量

$$
D_{KL}(p||q)=\sum_x p(x) \log \dfrac{p(x)}{q(x)}
$$

## 相互情報量

２つの情報が得られた事により、どれくらい不確実性が減ったかを示す量

$$
I(X, Y) = H(X)-H(X|Y)\\
= H(Y) - H(Y|X)\\
= H(X) + H(Y) - H(X, Y)
$$

- $I(X, Y)$: XとYの相互情報量
- $H(X)$: Xの自己エントロピー
- $H(Y)$: Yの自己エントロピー
- $H(X|Y)$: $Y$がわかった上での$X$の条件付きエントロピ
- $H(X, Y)$: $X, Y$の結合エントロピー