# SVM

**SVM(Support Vector Machine)**
: 分類にも回帰にも使用される教師あり学習アルゴリズム。

## 決定関数
２クラス分類問題で、どちらのクラスに属するかを判定するための関数

$$f(x) = w^Tx + b \ (wは特徴ベクトル)$$

SVMでは、決定関数を用いて

$$y=sgn \ f(x)=\begin{cases}
{+1 \ (f(x)>0)}\\
{-1 \ (f(x)<0)}\\
\end{cases}$$

のように符号の正負でクラスを識別する。

## サポートベクトル
マージン上のベクトルをサポートベクトルと呼ぶ。分類境界はサポートベクトルによって決定される。

## ハードマージンSVM
分類可能性を仮定したSVM

$$最小化する目的関数=\min_{w,b} \frac{1}{2}||w||^2$$

## ソフトマージンSVM
多少の誤りを許すことで、訓練データを完璧に分類する事のできる決定関数が存在しない、分類可能でないデータでも適用できるように拡張したもの

$$最小化する目的関数=\min_{w,b,\xi}[\frac{1}{2}||w||^2+C\sum^n_{i=1}\xi_i]$$

## スラック変数
ソフトマージンSVMにおけるスラック関数はマージン内に入ったデータや誤分類されたデータを表す変数。上の式の$\xi$

## 正則化係数
ソフトマージンSVMにおける正則化係数は、マージンに対する反応を制御する正の定数。上の式の$C$
Cが大きいとハードマージンに近づき、$C=\infty$だとハードマージンと等しくなる
Cが小さいと誤分類が許容されやすくなる

## カーネルトリック
多項式特徴量（高次元）、類似性特徴量を追加することで線形分離できない場合の性能を上げることができる。

それらの計算量を減らすため、ベクトルの内積をカーネル関数で置き換える手法をカーネルトリックと言う

## カーネル関数

$$K(X_i, X_j)=\psi(X_i)^T \psi(X_j)$$

## RBFカーネル
ガウスカーネルとも呼ぶ。

$$K(x, x') = exp(-\frac{1}{2\sigma^2}||x-x'||^2)$$

## 正規化線形ベクトル

$$K(x,x')=\dfrac{x^Tx}{||x||||x'||}$$

特徴ベクトル$\phi(x)$は

$$\phi(x)=\dfrac{x}{||x||}$$

## SVMのメリット

- データの次元が増加しても、識別精度が良い
- 最適化するパラメータが少ない
- 試行回数が少なくて済む
- 非線形にも対応できる
- 「マージン最大化」という識別のための明確な基準を持つ
- 未知のデータに対しても応用力が高い

## デメリット
- 学習データが増えると計算量が膨大
- 基本的に２クラス分類向けであること
- データの前処理が必要
パラメータは少ない一方、パラメーターの調整、結果の解釈が難しい

## sklearn

```python
# 線形SVMの場合
from sklearn.svm import LinearSVC
model1 = LinearSVC()

# 非線形SVMの場合
from sklearn.svm import SVC
model2 = SVC()
```

